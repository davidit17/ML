{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bd4fe5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:16.644690Z",
     "iopub.status.busy": "2025-04-06T04:49:16.644358Z",
     "iopub.status.idle": "2025-04-06T04:49:17.325433Z",
     "shell.execute_reply": "2025-04-06T04:49:17.324439Z"
    },
    "papermill": {
     "duration": 0.689664,
     "end_time": "2025-04-06T04:49:17.326909",
     "exception": false,
     "start_time": "2025-04-06T04:49:16.637245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mafat-mini/mini_training_set.db\n",
      "/kaggle/input/mafat-mini/training_set.db\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b294c466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:17.338557Z",
     "iopub.status.busy": "2025-04-06T04:49:17.338197Z",
     "iopub.status.idle": "2025-04-06T04:49:18.838744Z",
     "shell.execute_reply": "2025-04-06T04:49:18.837756Z"
    },
    "papermill": {
     "duration": 1.507607,
     "end_time": "2025-04-06T04:49:18.840138",
     "exception": false,
     "start_time": "2025-04-06T04:49:17.332531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03c4a8",
   "metadata": {
    "papermill": {
     "duration": 0.004927,
     "end_time": "2025-04-06T04:49:18.850539",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.845612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa6181b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.861670Z",
     "iopub.status.busy": "2025-04-06T04:49:18.861282Z",
     "iopub.status.idle": "2025-04-06T04:49:18.866697Z",
     "shell.execute_reply": "2025-04-06T04:49:18.866078Z"
    },
    "papermill": {
     "duration": 0.012088,
     "end_time": "2025-04-06T04:49:18.867763",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.855675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_extract(conn):\n",
    "    \"\"\"\n",
    "    This function return The Target of each Device_ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe of the labels.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    sql = '''SELECT DISTINCT Device_ID, Target\n",
    "             FROM data;\n",
    "             '''\n",
    "    cur.execute(sql)\n",
    "    target = pd.DataFrame(cur.fetchall(), columns = ['Device_ID', 'Target'])\n",
    "    target['Target'] = target['Target'].astype('int')\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb95c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.879086Z",
     "iopub.status.busy": "2025-04-06T04:49:18.878866Z",
     "iopub.status.idle": "2025-04-06T04:49:18.892681Z",
     "shell.execute_reply": "2025-04-06T04:49:18.891877Z"
    },
    "papermill": {
     "duration": 0.020955,
     "end_time": "2025-04-06T04:49:18.893987",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.873032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_and_16_convert(dataset,prefix):\n",
    "    \"\"\"\n",
    "    Processing data to reduce memory and creating unique columns names for features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: dataframe\n",
    "        A dataframe columns includes Device_ID and features to process.\n",
    "    prefix : str\n",
    "        A string to concatenate to the feature names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        The processed data inclued Device_ID column.\n",
    "    \"\"\"\n",
    "    col_dataset = list(dataset.drop(['Device_ID'], axis=1).columns)\n",
    "    df_device = dataset['Device_ID']\n",
    "\n",
    "    dataset.loc[:,col_dataset] *=1000\n",
    "    dataset = dataset.loc[:,col_dataset].astype('float16')\n",
    "    dataset = pd.concat([dataset, df_device], axis=1)\n",
    "\n",
    "    new_columns_name = {n: f'{prefix}_{n}'for n in col_dataset}\n",
    "\n",
    "    dataset.rename(columns=new_columns_name, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def relative_domain(conn, device_list):\n",
    "    \"\"\"\n",
    "    Feature engeinering: For each Device_ID calculate the proportions of all the domain_Name he entered.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "    device_list : list\n",
    "        A list of Device_IDs to calculate their proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe with the proportions for each Device_IDs and Domain_Name.\n",
    "    \"\"\"\n",
    "    list_device_str = ', '.join(map(str, device_list))\n",
    "    cur = conn.cursor()\n",
    "    sql = f'''SELECT DISTINCT\n",
    "                Device_ID,\n",
    "                Domain_Name,\n",
    "                Domain_Name_count*1.0 / SUM(Domain_Name_count) OVER (PARTITION BY Device_ID) AS relative_domain\n",
    "            FROM (\n",
    "              SELECT Device_ID, Domain_Name, COUNT(*) as Domain_Name_count\n",
    "            \t\tFROM data\n",
    "            \t\tWHERE Device_ID IN (''' +list_device_str+''')\n",
    "                    GROUP BY Device_ID, Domain_Name\n",
    "            \t\t) subquery;'''\n",
    "    cur.execute(sql)\n",
    "    df = pd.DataFrame(cur.fetchall(), columns = ['Device_ID','Domain_Name', 'relative_domain'])\n",
    "\n",
    "    df = df.pivot_table(index='Device_ID', columns='Domain_Name', values='relative_domain', fill_value=0)\n",
    "    df['Device_ID'] = df.index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "def cls_proportion(conn, device_list):\n",
    "    \"\"\"\n",
    "    Feature engeinering: For each Device_ID calculate the proportions of all the domain_cls he entered.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "    device_list : list\n",
    "        A list of Device_IDs to calculate their proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe with the proportions for each Device_IDs and Domain_cls.\n",
    "    \"\"\"\n",
    "    list_device_str = ', '.join(map(str, device_list))\n",
    "    cur = conn.cursor()\n",
    "    sql = '''SELECT\n",
    "                    Device_ID,\n",
    "                    Domain_cls,\n",
    "                    CAST(count_cls AS REAL) / SUM(count_cls) OVER (PARTITION BY Device_ID) AS proportion\n",
    "                    FROM\n",
    "                    (SELECT Device_ID, Domain_cls , COUNT(*) AS count_cls\n",
    "                    FROM (\n",
    "                        SELECT Device_ID, Domain_cls1 AS Domain_cls FROM data WHERE (Domain_cls1 != 0 AND Device_ID IN (''' +list_device_str+'''))\n",
    "                        UNION ALL\n",
    "                        SELECT Device_ID, Domain_cls2 AS Domain_cls FROM data WHERE (Domain_cls2 != 0 AND Device_ID IN (''' +list_device_str+'''))\n",
    "                        UNION ALL\n",
    "                        SELECT Device_ID, Domain_cls3 AS Domain_cls FROM data WHERE (Domain_cls3 != 0 AND Device_ID IN (''' +list_device_str+'''))\n",
    "                        UNION ALL\n",
    "                        SELECT Device_ID, Domain_cls4 AS Domain_cls FROM data WHERE (Domain_cls4 != 0 AND Device_ID IN (''' +list_device_str+'''))\n",
    "                    ) AS combined\n",
    "                    WHERE Domain_cls!=0\n",
    "                    GROUP BY Device_ID, Domain_cls\n",
    "                    ORDER BY Device_ID, Domain_cls)\n",
    "                    subquery;'''\n",
    "\n",
    "    cur.execute(sql)\n",
    "    df = pd.DataFrame(cur.fetchall(), columns = ['Device_ID','Domain_cls', 'proportion'])\n",
    "\n",
    "    df = df.pivot_table(index='Device_ID', columns='Domain_cls', values='proportion', fill_value=0)\n",
    "    df['Device_ID'] = df.index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "def avg_relative_entrances_device_id(conn, hours_duration, device_list):\n",
    "    \"\"\"\n",
    "    Feature engeinering: for each Device_ID calculation of the proportional hits according to the day's parts.\n",
    "    Calculation of proportional hits: For each Device_ID, sum up the proportional hits for each day's part (calculated each day) and divide them by the number of days (all days of internet usage -queries).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "    hours_duration : int\n",
    "        The interval duration of each day's parts in hours (Day division to 24/'hours_duration' parts).\n",
    "    device_list : list\n",
    "        A list of Device_IDs to calculate their proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe with the proportional hits for each Device_ID and time_range.\n",
    "    \"\"\"\n",
    "    df_sum_relative = sum_relative_entrances_timerange(conn, hours_duration, device_list)\n",
    "\n",
    "    all_desired_combinations = list(pd.MultiIndex.from_product([df_sum_relative['Device_ID'].unique(), range(int(24/hours_duration))], names=['Device_ID', 'time_range']))\n",
    "    diff_to_add = set(all_desired_combinations).difference(set(df_sum_relative.apply(lambda row: (row['Device_ID'], row['time_range']), axis=1).to_list()))\n",
    "    diff_to_add = [x +(0,) for x in diff_to_add]\n",
    "    diff_to_add_df = pd.DataFrame(diff_to_add, columns = list(df_sum_relative.columns))\n",
    "    df_sum_relative = pd.concat([df_sum_relative,diff_to_add_df], axis=0)\n",
    "    df_sum_relative.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    df_days_count_train = count_day_device_id(conn, device_list)\n",
    "\n",
    "    df = pd.merge(df_sum_relative, df_days_count_train, how ='left', on ='Device_ID')\n",
    "    df['relative_part'] = df['sum_relative_part']/df['day_num']\n",
    "    df.drop(['day_num','sum_relative_part'],axis=1, inplace = True)\n",
    "\n",
    "    df = df.pivot_table(index='Device_ID', columns='time_range', values='relative_part', fill_value=0)\n",
    "    df['Device_ID'] = df.index\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def sum_relative_entrances_timerange(conn, hours_duration, device_list):\n",
    "    \"\"\"\n",
    "    For each Device_ID, sum the proportional hits in each day according to the day's parts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "    hours_duration : int\n",
    "        The interval duration of each day's parts in hours (Day division to 24/'hours_duration' parts).\n",
    "    device_list:  list\n",
    "        A list of Device_IDs to calculate their proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe contains Device_ID, part of the day, and the sum of the proportional hits.\n",
    "    \"\"\"\n",
    "    list_device_str = ', '.join(map(str, device_list))\n",
    "    cur = conn.cursor()\n",
    "    sql = f'''SELECT DISTINCT\n",
    "                        Device_ID,\n",
    "                        time_range,\n",
    "                        SUM (relative_part) OVER (PARTITION BY Device_ID,time_range) AS sum_relative_part\n",
    "                    FROM(\n",
    "\n",
    "                            SELECT distinct\n",
    "                                            Device_ID,\n",
    "                                            date,\n",
    "                                            time_range,\n",
    "                                            CAST(COUNT(*) OVER (PARTITION BY Device_ID,date,time_range) AS REAL) / COUNT(*) OVER (PARTITION BY Device_ID,date) AS relative_part\n",
    "                                        FROM\n",
    "                                                    (SELECT\n",
    "                                                            Device_ID,\n",
    "                                                            Datetime,\n",
    "                                                            strftime('%Y-%m-%d', Datetime) AS date,\n",
    "                                                            (CAST(strftime('%H', Datetime) AS INTEGER) / {hours_duration}) AS time_range\n",
    "                                                        FROM\n",
    "                                                            data\n",
    "                                                        WHERE\n",
    "                                                            Device_ID IN (''' +list_device_str+''')\n",
    "                                                    ) subquery\n",
    "                        ) subquery\n",
    "                        ;'''\n",
    "    cur.execute(sql)\n",
    "    df = pd.DataFrame(cur.fetchall(), columns = ['Device_ID','time_range', 'sum_relative_part'])\n",
    "    return df\n",
    "\n",
    "def count_day_device_id(conn, device_list):\n",
    "    \"\"\"\n",
    "    This function counts the days with internet usage(queries) of each Device_ID.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: connection\n",
    "        A connection object to the database.\n",
    "    device_list:  list\n",
    "        A list of Device_IDs to calculate their proportions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe contains Device_ID and total days.\n",
    "    \"\"\"\n",
    "    list_device_str = ', '.join(map(str, device_list))\n",
    "    cur = conn.cursor()\n",
    "    sql = f'''\n",
    "                SELECT\n",
    "                    Device_ID,\n",
    "                    COUNT(DISTINCT strftime('%Y-%m-%d', Datetime)) AS day_num\n",
    "                FROM\n",
    "                    data\n",
    "                WHERE\n",
    "                    Device_ID IN (''' +list_device_str+''')\n",
    "                GROUP BY\n",
    "                    Device_ID\n",
    "                ;'''\n",
    "    cur.execute(sql)\n",
    "    df = pd.DataFrame(cur.fetchall(), columns = ['Device_ID', 'day_num'])\n",
    "    return df\n",
    "\n",
    "def corresponding_columns_training_set(df_train_col_list, df):\n",
    "    \"\"\"\n",
    "    This function checks the gaps between the features received as arguments and the data's columns. And changes the columns' data to be the same as those received as arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train_col_list: list\n",
    "        List of features from the training set\n",
    "    df:  dataframe\n",
    "        A dataset whose columns will be changed according to df_train_col_list.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        A dataframe with columns compatible with those of the training set.\n",
    "    \"\"\"\n",
    "    del_col = set(list(df.columns)) - set(df_train_col_list)\n",
    "    df.drop(columns = del_col, inplace = True)\n",
    "    diff_col = set(df_train_col_list)-set(list(df.columns))\n",
    "    add_to_test = pd.DataFrame(0, index=np.arange(len(df)), columns=list(diff_col)).astype('float16')\n",
    "    df = pd.concat([df, add_to_test], axis=1)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ac7fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.905015Z",
     "iopub.status.busy": "2025-04-06T04:49:18.904750Z",
     "iopub.status.idle": "2025-04-06T04:49:18.909762Z",
     "shell.execute_reply": "2025-04-06T04:49:18.908978Z"
    },
    "papermill": {
     "duration": 0.011821,
     "end_time": "2025-04-06T04:49:18.910965",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.899144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def basic_stats(conn, device_list):\n",
    " \n",
    "    list_device_str = ', '.join(map(str, device_list))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    sql = f'''\n",
    "    SELECT Device_ID,total_urls,unique_urls,total_dates,unique_domains,unique_cls1,\n",
    "    diff_mean,diff_std,diff_max,min_nonzero_cls,max_nonzero_cls,mean_nonzero_cls,std_nonzero_cls,\n",
    "\n",
    " sum_equal_0,sum_equal_1,sum_equal_2,sum_equal_3,\n",
    "\n",
    " (CAST (sum_equal_0 as REAL)/total_urls) as sum_equal_0_perc,\n",
    " (CAST (sum_equal_1 as REAL)/total_urls) as sum_equal_1_perc,\n",
    " (CAST (sum_equal_2 as REAL)/total_urls) as sum_equal_2_perc,\n",
    " (CAST (sum_equal_3 as REAL)/total_urls) as sum_equal_3_perc\n",
    "\n",
    "      FROM ( SELECT \n",
    "        d.Device_ID,\n",
    "        COUNT(d.URL) AS total_urls,\n",
    "        \n",
    "        COUNT(DISTINCT d.URL) AS unique_urls,\n",
    "        COUNT(DISTINCT d.Datetime) AS total_dates,\n",
    "        \n",
    "        COUNT(DISTINCT d.Domain_Name) AS unique_domains,\n",
    "        COUNT(DISTINCT d.Domain_cls1) AS unique_cls1,\n",
    "        \n",
    "        AVG(diff) AS diff_mean,\n",
    "        SQRT(AVG(diff * diff) - (AVG(diff) * AVG(diff))) AS diff_std,\n",
    "        MAX(diff) AS diff_max,\n",
    "\n",
    "        MIN(nonzero_csl) as min_nonzero_cls,\n",
    "        MAX(nonzero_csl) as max_nonzero_cls,\n",
    "        AVG(nonzero_csl) as mean_nonzero_cls,\n",
    "        SQRT(AVG(nonzero_csl * nonzero_csl) - (AVG(nonzero_csl) * AVG(nonzero_csl))) as std_nonzero_cls,\n",
    "\n",
    "        SUM( CASE WHEN nonzero_csl =0 THEN 1 ELSE 0 END)  as sum_equal_0,\n",
    "        SUM( CASE WHEN nonzero_csl =1 THEN 1 ELSE 0 END)  as sum_equal_1,\n",
    "        SUM( CASE WHEN nonzero_csl =2 THEN 1 ELSE 0 END)  as sum_equal_2,\n",
    "        SUM( CASE WHEN nonzero_csl =3 THEN 1 ELSE 0 END)  as sum_equal_3\n",
    "        \n",
    "        FROM (\n",
    "        SELECT \n",
    "            Device_ID,\n",
    "            Datetime,\n",
    "            URL,\n",
    "            Domain_Name,\n",
    "            Domain_cls1,\n",
    "            (CASE WHEN Domain_cls1 != 0 THEN 1 ELSE 0 END +\n",
    "            CASE WHEN Domain_cls2 != 0 THEN 1 ELSE 0 END +\n",
    "            CASE WHEN Domain_cls3 != 0 THEN 1 ELSE 0 END +\n",
    "            CASE WHEN Domain_cls4 != 0 THEN 1 ELSE 0 END) AS nonzero_csl,\n",
    "           \n",
    "            CAST((strftime('%s', Datetime) - strftime('%s', LAG(Datetime) OVER (PARTITION BY Device_ID ORDER BY Datetime))) AS REAL) AS diff\n",
    "            FROM data WHERE Device_ID IN (''' +list_device_str+''')\n",
    "    ) AS d\n",
    "    GROUP BY d.Device_ID) as before_normalized\n",
    "    \n",
    "    ;\n",
    "    '''\n",
    "    cur.execute(sql)\n",
    "    df = pd.DataFrame(cur.fetchall(), columns = ['Device_ID','total_urls', 'unique_urls','total_dates',# 'min_date', 'max_date',\n",
    "         'unique_domains', 'unique_cls1', 'diff_mean', 'diff_std', 'diff_max','min_nonzero_cls',\n",
    "            'max_nonzero_cls','mean_nonzero_cls','std_nonzero_cls','sum_equal_0','sum_equal_1',\n",
    "               'sum_equal_2', 'sum_equal_3','sum_equal_0_perc','sum_equal_1_perc','sum_equal_2_perc','sum_equal_3_perc' ])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0b01b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.921693Z",
     "iopub.status.busy": "2025-04-06T04:49:18.921448Z",
     "iopub.status.idle": "2025-04-06T04:49:18.927311Z",
     "shell.execute_reply": "2025-04-06T04:49:18.926687Z"
    },
    "papermill": {
     "duration": 0.012493,
     "end_time": "2025-04-06T04:49:18.928475",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.915982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def acf_on_group(x):\n",
    "    acf_result = acf(x['total_urls'])\n",
    "\n",
    "    funcs = ['mean','std','min','max','median']\n",
    "\n",
    "    autocorr = pd.Series(acf_result).agg(funcs)\n",
    "    autocorr.index = 'acf_'+ autocorr.index\n",
    "\n",
    "    smoothing_factor = 0.3\n",
    "    smoothed_ts = x['total_urls'].ewm(alpha=smoothing_factor,adjust=False).mean()\n",
    "    sm_agg = pd.Series(smoothed_ts).agg(funcs)\n",
    "    sm_agg.index = 'smoothing_'+ sm_agg.index\n",
    "\n",
    "\n",
    "    ent =  pd.Series(entropy(x.value_counts(), base=2),index=['enthropy'])\n",
    "\n",
    "    return pd.Series(pd.concat([autocorr,sm_agg,ent]))\n",
    "\n",
    "\n",
    "def seasonal_deco(x,period):\n",
    "    stl_result = STL(x['total_urls'],period = period).fit()\n",
    "    period = str(period)\n",
    "\n",
    "    funcs = ['mean','std','min','max',\"median\"]\n",
    "    \n",
    "    trend = stl_result.trend.agg(funcs)\n",
    "    trend.index = period +'_trend_'+ trend.index\n",
    "\n",
    "    seasonal = stl_result.seasonal.agg(funcs)\n",
    "    seasonal.index = period +'_seasonal_'+ seasonal.index\n",
    "\n",
    "    resid = stl_result.resid.agg(funcs)\n",
    "    resid.index = period +'_resid_'+ resid.index\n",
    "\n",
    "\n",
    "\n",
    "    return pd.Series(pd.concat([trend,seasonal,resid]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7890fe61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.939196Z",
     "iopub.status.busy": "2025-04-06T04:49:18.938994Z",
     "iopub.status.idle": "2025-04-06T04:49:18.947781Z",
     "shell.execute_reply": "2025-04-06T04:49:18.947158Z"
    },
    "papermill": {
     "duration": 0.015381,
     "end_time": "2025-04-06T04:49:18.948880",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.933499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def resampled_features(chunk_df):\n",
    "    cols = [ 'Datetime','Device_ID',\n",
    "        'URL',\n",
    "        'Domain_Name',\n",
    "        'Domain_cls1']\n",
    "    chunk_df = chunk_df[cols].copy()\n",
    "\n",
    "    \n",
    "    #### repeating values\n",
    "    \n",
    "    chunk_df['same_Datetime_as_previous'] = chunk_df['Datetime'] == chunk_df['Datetime'].shift(1)\n",
    "    chunk_df['same_URL_as_previous'] = chunk_df['URL'] == chunk_df['URL'].shift(1)\n",
    "    chunk_df['same_Domain_Name_as_previous'] = chunk_df['Domain_Name'] == chunk_df['Domain_Name'].shift(1)\n",
    "    chunk_df['same_Domain_cls1_as_previous'] = (chunk_df['Domain_cls1'] == chunk_df['Domain_cls1'].shift(1))&(chunk_df['Domain_cls1']!=0)\n",
    "    \n",
    "    \n",
    "    repeating_data = chunk_df.groupby('Device_ID',as_index=True).agg(total_urls = ('same_Datetime_as_previous','sum'),\n",
    "                        repeating_url = ('same_URL_as_previous','sum'),\n",
    "                        repeating_domain_name = ('same_Domain_Name_as_previous','sum'),\n",
    "                        repeating_cls1 = ('same_Domain_cls1_as_previous','sum'),\n",
    "    \n",
    "    )\n",
    "\n",
    "    ## resampled data\n",
    "\n",
    "    chunk_df[\"Datetime\"] = pd.to_datetime(chunk_df[\"Datetime\"])\n",
    "    \n",
    "    agg_by_datetime = chunk_df.groupby(['Device_ID','Datetime'],as_index=False).agg(total_urls = ('URL','count'),\n",
    "                    unique_domains = ('Domain_Name','nunique'),\n",
    "                    unique_cls = ('Domain_cls1','nunique'),)\n",
    "\n",
    "    del chunk_df\n",
    "    \n",
    "    #resample\n",
    "    \n",
    "    daily_level = agg_by_datetime.groupby(['Device_ID',pd.Grouper(freq='D', key='Datetime')])\\\n",
    "                            .agg(['sum','mean','std']).reset_index()\n",
    "    \n",
    "    daily_level.columns = [''.join(col) if (col[1] == '') else '_'.join(col) for col in daily_level.columns]\n",
    "    \n",
    "    daily_level = daily_level.drop('Datetime',axis=1)\n",
    "    daily_level = daily_level.groupby('Device_ID').agg(['mean','std'])\n",
    "    daily_level.columns = ['_daily_'.join(col) for col in daily_level.columns]\n",
    "    \n",
    "\n",
    "    ### resample week\n",
    "\n",
    "    weekly_level = agg_by_datetime.groupby(['Device_ID',pd.Grouper(freq='W', key='Datetime')])\\\n",
    "                            .agg(['sum','mean','std']).reset_index()\n",
    "    \n",
    "    weekly_level.columns = [''.join(col) if (col[1] == '') else '_'.join(col) for col in weekly_level.columns]\n",
    "    \n",
    "    weekly_level = weekly_level.drop('Datetime',axis=1)\n",
    "    weekly_level = weekly_level.groupby('Device_ID').agg(['mean','std'])\n",
    "    weekly_level.columns = ['_weekly_'.join(col) for col in weekly_level.columns]\n",
    "\n",
    "\n",
    "\n",
    "    resampled_df = pd.concat([daily_level,weekly_level,repeating_data],axis=1)\n",
    "\n",
    "    \n",
    "    acf_df = agg_by_datetime.groupby('Device_ID').apply(acf_on_group,include_groups=False)\n",
    "    seasonal_decomp_7 = agg_by_datetime.groupby('Device_ID').apply(seasonal_deco,period = 7,include_groups=False)\n",
    "    seasonal_decomp_24 = agg_by_datetime.groupby('Device_ID').apply(seasonal_deco,period = 24,include_groups=False)\n",
    "    seasonal_decomp_30 = agg_by_datetime.groupby('Device_ID').apply(seasonal_deco,period = 30,include_groups=False)\n",
    "    \n",
    "    seasonal_df = pd.concat([acf_df,seasonal_decomp_7,seasonal_decomp_24,seasonal_decomp_30],axis=1)\n",
    "\n",
    "    \n",
    "    current_chunk_df = pd.concat([resampled_df,seasonal_df],axis=1).reset_index(drop=True)\n",
    "    return current_chunk_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e3e3a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:18.960269Z",
     "iopub.status.busy": "2025-04-06T04:49:18.960064Z",
     "iopub.status.idle": "2025-04-06T04:49:19.313904Z",
     "shell.execute_reply": "2025-04-06T04:49:19.313243Z"
    },
    "papermill": {
     "duration": 0.36133,
     "end_time": "2025-04-06T04:49:19.315354",
     "exception": false,
     "start_time": "2025-04-06T04:49:18.954024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.stats import entropy\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "def load_data_in_chunks(device_list, chunk_size=650):\n",
    "    conn = sqlite3.connect(db)\n",
    "    \n",
    "    for i in range(0, len(device_list), chunk_size):\n",
    "        chunk = device_list[i:i + chunk_size]  # Get the next batch of devices\n",
    "        list_device_str = ', '.join(map(str, chunk))\n",
    "        query = f'''SELECT * FROM data WHERE Device_ID IN (''' +list_device_str+''')'''\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        yield df  \n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db287f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:19.326944Z",
     "iopub.status.busy": "2025-04-06T04:49:19.326566Z",
     "iopub.status.idle": "2025-04-06T04:49:19.335004Z",
     "shell.execute_reply": "2025-04-06T04:49:19.334358Z"
    },
    "papermill": {
     "duration": 0.015318,
     "end_time": "2025-04-06T04:49:19.336171",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.320853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weekly_pivoted(chunk_df):\n",
    "\n",
    "    chunk_df[\"Datetime\"] = pd.to_datetime(chunk_df[\"Datetime\"])\n",
    "    \n",
    "    agg_by_datetime = chunk_df.groupby(['Device_ID','Datetime'],as_index=False).agg(total_urls = ('URL','count'),\n",
    "                    unique_domains = ('Domain_Name','nunique'),\n",
    "                    unique_cls = ('Domain_cls1','nunique'),)\n",
    "\n",
    "    del chunk_df\n",
    "\n",
    "    funcs = ['mean','std','min','max',\"median\"]\n",
    "\n",
    "    week = agg_by_datetime.groupby(['Device_ID',pd.Grouper(freq='W', key='Datetime')])\\\n",
    "                                    .agg(funcs).reset_index()\n",
    "    \n",
    "    week.columns = [''.join(col) if (col[1] == '') else '_'.join(col) for col in week.columns]\n",
    "    week['week_num'] = ((week[\"Datetime\"].dt.day - 1) // 7 + 1)\n",
    "    week['week_num'] = week['week_num'].map({4:1,5:2,1:3,2:4}).astype(str)\n",
    "    \n",
    "    ### fill missing weeks\n",
    "    \n",
    "    all_weeks = pd.DataFrame({'week_num': ['1', '2', '3', '4']})\n",
    "    device_ids = week['Device_ID'].unique()\n",
    "    full_index = pd.MultiIndex.from_product([device_ids, all_weeks['week_num']], names=['Device_ID', 'week_num'])\n",
    "    filled_week = week.set_index(['Device_ID', 'week_num']).reindex(full_index).reset_index()\n",
    "    \n",
    "    filled_week = filled_week.fillna(0)\n",
    "\n",
    "\n",
    "    columns_to_pivot = ['total_urls_mean', 'total_urls_std',\n",
    "           'total_urls_min', 'total_urls_max', 'total_urls_median',\n",
    "           'unique_domains_mean', 'unique_domains_std', 'unique_domains_min',\n",
    "           'unique_domains_max', 'unique_domains_median', 'unique_cls_mean',\n",
    "           'unique_cls_std', 'unique_cls_min', 'unique_cls_max',\n",
    "           'unique_cls_median',]\n",
    "    \n",
    "    \n",
    "    pivoted_week = filled_week.pivot(index='Device_ID', columns='week_num', values=columns_to_pivot)\n",
    "    pivoted_week.columns = ['_week_'.join(col) for col in pivoted_week.columns]\n",
    "\n",
    "    # add diff\n",
    "    diff_cols = ['Device_ID', 'week_num']\n",
    "    \n",
    "    for col in columns_to_pivot:\n",
    "        filled_week[col + '_diff'] = filled_week.groupby('Device_ID')[col].diff()\n",
    "        filled_week[col + '_pct_change'] = filled_week.groupby('Device_ID')[col].pct_change()\n",
    "        diff_cols.append(col + '_diff')\n",
    "        diff_cols.append(col + '_pct_change')\n",
    "    \n",
    "    diff_week_df = filled_week[diff_cols]\n",
    "    diff_week_df = diff_week_df[diff_week_df['week_num']!='1']\n",
    "\n",
    "\n",
    "    diff_columns_to_pivot = ['total_urls_mean_diff', 'total_urls_std_diff',\n",
    "           'total_urls_min_diff', 'total_urls_max_diff', 'total_urls_median_diff',\n",
    "           'unique_domains_mean_diff', 'unique_domains_std_diff',\n",
    "           'unique_domains_min_diff', 'unique_domains_max_diff',\n",
    "           'unique_domains_median_diff', 'unique_cls_mean_diff',\n",
    "           'unique_cls_std_diff', 'unique_cls_min_diff', 'unique_cls_max_diff',\n",
    "           'unique_cls_median_diff']\n",
    "    \n",
    "    \n",
    "    pivoted_diff = diff_week_df.pivot(index='Device_ID', columns='week_num', values=diff_columns_to_pivot)\n",
    "    pivoted_diff.columns = ['_week_'.join(col) for col in pivoted_diff.columns]\n",
    "\n",
    "    weekly_pivoted_df = pd.concat([pivoted_week,pivoted_diff],axis=1)\n",
    "    return weekly_pivoted_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6454b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:19.347011Z",
     "iopub.status.busy": "2025-04-06T04:49:19.346787Z",
     "iopub.status.idle": "2025-04-06T04:49:19.351463Z",
     "shell.execute_reply": "2025-04-06T04:49:19.350867Z"
    },
    "papermill": {
     "duration": 0.011329,
     "end_time": "2025-04-06T04:49:19.352649",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.341320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.37.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlite3.sqlite_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac639ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:19.363645Z",
     "iopub.status.busy": "2025-04-06T04:49:19.363395Z",
     "iopub.status.idle": "2025-04-06T04:49:19.371543Z",
     "shell.execute_reply": "2025-04-06T04:49:19.370865Z"
    },
    "papermill": {
     "duration": 0.014816,
     "end_time": "2025-04-06T04:49:19.372620",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.357804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini : False\n"
     ]
    }
   ],
   "source": [
    "#mini = True\n",
    "mini = False\n",
    "\n",
    "print('mini :',mini)\n",
    "\n",
    "\n",
    "if mini:\n",
    "    db = '/kaggle/input/mafat-mini/mini_training_set.db'\n",
    "else:\n",
    "    db = '/kaggle/input/mafat-mini/training_set.db'\n",
    "\n",
    "conn = sqlite3.connect(db)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a5dca",
   "metadata": {
    "papermill": {
     "duration": 0.004966,
     "end_time": "2025-04-06T04:49:19.382865",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.377899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# run program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30aec36a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:19.393990Z",
     "iopub.status.busy": "2025-04-06T04:49:19.393769Z",
     "iopub.status.idle": "2025-04-06T04:49:19.396911Z",
     "shell.execute_reply": "2025-04-06T04:49:19.396275Z"
    },
    "papermill": {
     "duration": 0.010015,
     "end_time": "2025-04-06T04:49:19.398163",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.388148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN = 1\n",
    "GET_TARGET = 1 \n",
    "CRAETE_BASIC_PANEL = 0\n",
    "CREATE_DAILY_PANEL = 1\n",
    "CREATE_WEEKLY_PANEL = 1\n",
    "\n",
    "use_n_devices = False\n",
    "#use_n_devices = 100\n",
    "\n",
    "\n",
    "SAVE_PANEL = 1\n",
    "assert SAVE_PANEL,''' SAVE PANEL IS OFF!!!!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8497eee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:49:19.411551Z",
     "iopub.status.busy": "2025-04-06T04:49:19.411339Z",
     "iopub.status.idle": "2025-04-06T04:53:07.361357Z",
     "shell.execute_reply": "2025-04-06T04:53:07.360271Z"
    },
    "papermill": {
     "duration": 227.964811,
     "end_time": "2025-04-06T04:53:07.368115",
     "exception": false,
     "start_time": "2025-04-06T04:49:19.403304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6154\n",
      "CPU times: user 51.5 s, sys: 14 s, total: 1min 5s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "device_list = pd.read_sql_query(\"SELECT distinct Device_ID FROM data \", conn)[\"Device_ID\"].tolist()\n",
    "if use_n_devices :\n",
    "    device_list = device_list[:use_n_devices]\n",
    "print(len(device_list))\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fee95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:53:07.379926Z",
     "iopub.status.busy": "2025-04-06T04:53:07.379664Z",
     "iopub.status.idle": "2025-04-06T04:56:49.439603Z",
     "shell.execute_reply": "2025-04-06T04:56:49.438525Z"
    },
    "papermill": {
     "duration": 222.073027,
     "end_time": "2025-04-06T04:56:49.446691",
     "exception": false,
     "start_time": "2025-04-06T04:53:07.373664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the target variable\n",
      "Getting indexes for a partition that preserves the proportions of the data\n"
     ]
    }
   ],
   "source": [
    "if GET_TARGET:\n",
    "    print('Extract the target variable')\n",
    "    target_df = target_extract(conn)\n",
    "\n",
    "    print('Getting indexes for a partition that preserves the proportions of the data')\n",
    "    s = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in s.split(target_df.Target.values,target_df.Target.values):\n",
    "        train_target = target_df.iloc[train_index,:]\n",
    "        test_target = target_df.iloc[test_index,:]\n",
    "    \n",
    "        train_device = list(train_target.Device_ID)\n",
    "        test_device = list(test_target.Device_ID)\n",
    "\n",
    "    hours_duration = \"3\" # @param [\"2\", \"3\", \"4\", \"6\", \"8\"]\n",
    "    hours_duration = int(hours_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58177979",
   "metadata": {
    "papermill": {
     "duration": 0.005235,
     "end_time": "2025-04-06T04:56:49.457462",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.452227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## prepare panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c4aa7",
   "metadata": {
    "papermill": {
     "duration": 0.005167,
     "end_time": "2025-04-06T04:56:49.467959",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.462792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### basic panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dbed5ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:56:49.479461Z",
     "iopub.status.busy": "2025-04-06T04:56:49.479226Z",
     "iopub.status.idle": "2025-04-06T04:56:49.484839Z",
     "shell.execute_reply": "2025-04-06T04:56:49.484034Z"
    },
    "papermill": {
     "duration": 0.012756,
     "end_time": "2025-04-06T04:56:49.486070",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.473314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if CRAETE_BASIC_PANEL:\n",
    "    print('CRAETE_BASIC_PANEL')\n",
    "    domain_name_feat_train = relative_domain(conn, train_device)\n",
    "    domain_name_feat_train = rename_and_16_convert(domain_name_feat_train,'Domain')\n",
    "    \n",
    "    cls_name_feat_train = cls_proportion(conn, train_device)\n",
    "    cls_name_feat_train = rename_and_16_convert(cls_name_feat_train,'cls')\n",
    "    \n",
    "    ts_feat_train = avg_relative_entrances_device_id(conn, hours_duration, train_device)\n",
    "    ts_feat_train = rename_and_16_convert(ts_feat_train,'ts')\n",
    "\n",
    "    basic_stats_feat_train = basic_stats(conn,train_device)\n",
    "    \n",
    "    df_train = pd.merge(domain_name_feat_train, cls_name_feat_train, how ='left', on ='Device_ID')\n",
    "    df_train = pd.merge(df_train, ts_feat_train, how ='left', on ='Device_ID')\n",
    "    df_train = pd.merge(df_train, basic_stats_feat_train, how ='left', on ='Device_ID')\n",
    "\n",
    "    df_train_columns = list(df_train.columns)\n",
    "    \n",
    "    \n",
    "    df_train.to_parquet('df_train', index=False, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17486f68",
   "metadata": {
    "papermill": {
     "duration": 0.005261,
     "end_time": "2025-04-06T04:56:49.496716",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.491455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### basic panel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3442206c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:56:49.508395Z",
     "iopub.status.busy": "2025-04-06T04:56:49.508108Z",
     "iopub.status.idle": "2025-04-06T04:56:49.514478Z",
     "shell.execute_reply": "2025-04-06T04:56:49.513592Z"
    },
    "papermill": {
     "duration": 0.013637,
     "end_time": "2025-04-06T04:56:49.515763",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.502126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if CRAETE_BASIC_PANEL:\n",
    "    print('CRAETE_BASIC_PANEL_test')\n",
    "    domain_name_feat_test = relative_domain(conn, test_device)\n",
    "    domain_name_feat_test = rename_and_16_convert(domain_name_feat_test,'Domain')\n",
    "    \n",
    "    cls_name_feat_test = cls_proportion(conn, test_device)\n",
    "    cls_name_feat_test = rename_and_16_convert(cls_name_feat_test,'cls')\n",
    "    \n",
    "    ts_feat_test = avg_relative_entrances_device_id(conn, hours_duration, test_device)\n",
    "    ts_feat_test = rename_and_16_convert(ts_feat_test,'ts')\n",
    "\n",
    "    basic_stats_feat_test = basic_stats(conn,test_device)\n",
    "    \n",
    "    df_test = pd.merge(domain_name_feat_test, cls_name_feat_test, how ='left', on ='Device_ID')\n",
    "    df_test = pd.merge(df_test, ts_feat_test, how ='left', on ='Device_ID')\n",
    "    df_test = pd.merge(df_test, basic_stats_feat_test, how ='left', on ='Device_ID')\n",
    "    \n",
    "    df_test = corresponding_columns_training_set(list(df_train.columns), df_test)\n",
    "    \n",
    "    df_test.to_parquet('df_test', index=False, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b4142",
   "metadata": {
    "papermill": {
     "duration": 0.005307,
     "end_time": "2025-04-06T04:56:49.526599",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.521292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9ec1e0",
   "metadata": {
    "papermill": {
     "duration": 0.00539,
     "end_time": "2025-04-06T04:56:49.537562",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.532172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## load panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2a9444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:56:49.549419Z",
     "iopub.status.busy": "2025-04-06T04:56:49.549127Z",
     "iopub.status.idle": "2025-04-06T04:56:49.553403Z",
     "shell.execute_reply": "2025-04-06T04:56:49.552652Z"
    },
    "papermill": {
     "duration": 0.011751,
     "end_time": "2025-04-06T04:56:49.554757",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.543006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not GET_TARGET and CRAETE_BASIC_PANEL:\n",
    "    path = '/kaggle/input/panels-and-basic-features-v2/'\n",
    "   \n",
    "    df_train = pd.read_parquet(path+'df_train', engine=\"pyarrow\")\n",
    "\n",
    "    df_test = pd.read_parquet(path+'df_test', engine=\"pyarrow\")\n",
    "\n",
    "    df_train_columns = list(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d618a8",
   "metadata": {
    "papermill": {
     "duration": 0.005235,
     "end_time": "2025-04-06T04:56:49.565750",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.560515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35436090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:56:49.577571Z",
     "iopub.status.busy": "2025-04-06T04:56:49.577276Z",
     "iopub.status.idle": "2025-04-06T04:56:49.581912Z",
     "shell.execute_reply": "2025-04-06T04:56:49.581133Z"
    },
    "papermill": {
     "duration": 0.011913,
     "end_time": "2025-04-06T04:56:49.583091",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.571178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if GET_TARGET and CRAETE_BASIC_PANEL:\n",
    "    train_target.to_parquet('train_target', index=False, engine=\"pyarrow\")\n",
    "    test_target.to_parquet('test_target', index=False, engine=\"pyarrow\")\n",
    "\n",
    "if not GET_TARGET and CRAETE_BASIC_PANEL:\n",
    "    train_target = pd.read_parquet('train_target', engine=\"pyarrow\")\n",
    "    test_target = pd.read_parquet('test_target', engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89a20b",
   "metadata": {
    "papermill": {
     "duration": 0.00547,
     "end_time": "2025-04-06T04:56:49.594074",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.588604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## daily features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e427f89",
   "metadata": {
    "papermill": {
     "duration": 0.005337,
     "end_time": "2025-04-06T04:56:49.604924",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.599587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53d8768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T04:56:49.616796Z",
     "iopub.status.busy": "2025-04-06T04:56:49.616489Z",
     "iopub.status.idle": "2025-04-06T08:19:14.832420Z",
     "shell.execute_reply": "2025-04-06T08:19:14.831488Z"
    },
    "papermill": {
     "duration": 12145.228831,
     "end_time": "2025-04-06T08:19:14.839189",
     "exception": false,
     "start_time": "2025-04-06T04:56:49.610358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_DAILY_PANEL\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "(6154, 96)\n",
      "CPU times: user 2h 51min 34s, sys: 5min 44s, total: 2h 57min 18s\n",
      "Wall time: 3h 22min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if CREATE_DAILY_PANEL:\n",
    "    print('CREATE_DAILY_PANEL')\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for chunk_df in load_data_in_chunks(device_list, chunk_size=600):\n",
    "        print('new chunk')\n",
    "    \n",
    "        ### agg \n",
    "    \n",
    "        cols = ['Device_ID',\n",
    "                'Datetime',\n",
    "                'URL',\n",
    "                'Domain_Name',\n",
    "                'Domain_cls1']\n",
    "        chunk_df = chunk_df[cols].copy()\n",
    "    \n",
    "        current_chunk_df = resampled_features(chunk_df)\n",
    "\n",
    "        del chunk_df\n",
    "    \n",
    "        final_df = pd.concat([final_df, current_chunk_df], ignore_index=False)\n",
    "    \n",
    "    \n",
    "    if SAVE_PANEL:\n",
    "        print(final_df.shape)\n",
    "        final_df.to_parquet('new_features_df', index=False, engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ab100c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T08:19:14.853710Z",
     "iopub.status.busy": "2025-04-06T08:19:14.853401Z",
     "iopub.status.idle": "2025-04-06T08:19:14.858196Z",
     "shell.execute_reply": "2025-04-06T08:19:14.857488Z"
    },
    "papermill": {
     "duration": 0.013535,
     "end_time": "2025-04-06T08:19:14.859361",
     "exception": false,
     "start_time": "2025-04-06T08:19:14.845826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CREATE_DAILY_PANEL and CRAETE_BASIC_PANEL :\n",
    "    df_train = pd.merge(df_train, final_df, how ='left', on ='Device_ID')\n",
    "    \n",
    "    df_test = pd.merge(df_test, final_df, how ='left', on ='Device_ID')\n",
    "    \n",
    "    if SAVE_PANEL:\n",
    "        print('save added features')\n",
    "        df_train.to_parquet('df_train_added_features_final', index=False, engine=\"pyarrow\")\n",
    "        df_test.to_parquet('df_test_added_features_final', index=False, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "245fdc30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T08:19:14.873293Z",
     "iopub.status.busy": "2025-04-06T08:19:14.873067Z",
     "iopub.status.idle": "2025-04-06T09:31:37.478869Z",
     "shell.execute_reply": "2025-04-06T09:31:37.477940Z"
    },
    "papermill": {
     "duration": 4342.614248,
     "end_time": "2025-04-06T09:31:37.480287",
     "exception": false,
     "start_time": "2025-04-06T08:19:14.866039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE_WEEKLY_PANEL\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "new chunk\n",
      "final panel\n",
      "(6154, 105)\n",
      "CPU times: user 40min 33s, sys: 5min 19s, total: 45min 53s\n",
      "Wall time: 1h 12min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if CREATE_WEEKLY_PANEL:\n",
    "    print('CREATE_WEEKLY_PANEL')\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for chunk_df in load_data_in_chunks(device_list, chunk_size=600):\n",
    "        print('new chunk')\n",
    "    \n",
    "        ### agg \n",
    "    \n",
    "        cols = ['Device_ID',\n",
    "                'Datetime',\n",
    "                'URL',\n",
    "                'Domain_Name',\n",
    "                'Domain_cls1']\n",
    "        chunk_df = chunk_df[cols].copy()\n",
    "\n",
    "        current_chunk_df = weekly_pivoted(chunk_df)\n",
    "\n",
    "        del chunk_df\n",
    "\n",
    "        final_df = pd.concat([final_df, current_chunk_df], ignore_index=False)\n",
    "    \n",
    "    \n",
    "    if SAVE_PANEL:\n",
    "        print('final panel')\n",
    "        print(final_df.shape)\n",
    "        final_df.to_parquet('new_features_df_set2', index=False, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a17743b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:31:37.495807Z",
     "iopub.status.busy": "2025-04-06T09:31:37.495498Z",
     "iopub.status.idle": "2025-04-06T09:31:37.500094Z",
     "shell.execute_reply": "2025-04-06T09:31:37.499225Z"
    },
    "papermill": {
     "duration": 0.013676,
     "end_time": "2025-04-06T09:31:37.501411",
     "exception": false,
     "start_time": "2025-04-06T09:31:37.487735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CREATE_WEEKLY_PANEL and CREATE_DAILY_PANEL and CRAETE_BASIC_PANEL :\n",
    "    df_train = pd.merge(df_train, final_df, how ='left', on ='Device_ID')\n",
    "    \n",
    "    df_test = pd.merge(df_test, final_df, how ='left', on ='Device_ID')\n",
    "\n",
    "    print(df_train.shape)\n",
    "    print(df_test.shape)\n",
    "    \n",
    "    if SAVE_PANEL:\n",
    "        print('save added features')\n",
    "        df_train.to_parquet('df_train_added_features_final', index=False, engine=\"pyarrow\")\n",
    "        df_test.to_parquet('df_test_added_features_final', index=False, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730253ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:31:37.516084Z",
     "iopub.status.busy": "2025-04-06T09:31:37.515864Z",
     "iopub.status.idle": "2025-04-06T09:31:37.519280Z",
     "shell.execute_reply": "2025-04-06T09:31:37.518704Z"
    },
    "papermill": {
     "duration": 0.011961,
     "end_time": "2025-04-06T09:31:37.520386",
     "exception": false,
     "start_time": "2025-04-06T09:31:37.508425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6679788,
     "sourceId": 10771621,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16944.186038,
   "end_time": "2025-04-06T09:31:38.162059",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-06T04:49:13.976021",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
